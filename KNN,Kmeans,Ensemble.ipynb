{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KNN,Kmeans,Ensemble.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQX2FV0yEQpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "downloaded = drive.CreateFile({'id':'11y79Tmi7znU_ancRPVVn5MWffJ1i_9G0'}) # replace the id with id of file that you want to access\n",
        "downloaded.GetContentFile('data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "395FPVb9EWha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn import preprocessing\n",
        "from sklearn import model_selection\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
        "from sklearn import metrics\n",
        "from numpy import array\n",
        "from numpy import mean\n",
        "from numpy import cov\n",
        "from numpy.linalg import eig\n",
        "from copy import deepcopy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SP6HKGebEsDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Accuracy(y_true,y_pred):\n",
        "    \"\"\"\n",
        "    :type y_true: numpy.ndarray\n",
        "    :type y_pred: numpy.ndarray\n",
        "    :rtype: float\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    rightly_pred = 0\n",
        "    for i in range(len(y_true)):\n",
        "      if y_true[i]==y_pred[i]:\n",
        "        rightly_pred = rightly_pred+1\n",
        "    accuracy = rightly_pred/(len(y_true))\n",
        "    return accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSCCBDxfEsmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Recall(y_true,y_pred):\n",
        "    \"\"\"\n",
        "    :type y_true: numpy.ndarray\n",
        "    :type y_pred: numpy.ndarray\n",
        "    :rtype: float\n",
        "    \"\"\"\n",
        "    #using confusion matrix to calculate recall\n",
        "    x = ConfusionMatrix(y_true,y_pred)\n",
        "    diag = np.diagonal(x)\n",
        "    rec = 0\n",
        "    for i in range(len(diag)):\n",
        "      if diag[i] == 0:\n",
        "        rec = rec + 0\n",
        "      else:\n",
        "        rec = rec + (diag[i])/(np.sum(x[i]))\n",
        "    recall = rec / len(diag)\n",
        "    return recall"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK4eFtiSEvus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Precision(y_true,y_pred):\n",
        "    \"\"\"\n",
        "    :type y_true: numpy.ndarray\n",
        "    :type y_pred: numpy.ndarray\n",
        "    :rtype: float\n",
        "    \"\"\"\n",
        "    #Using confusion matrix to calculate precision\n",
        "    x = ConfusionMatrix(y_true,y_pred)\n",
        "    diag = np.diagonal(x)\n",
        "    prec = 0\n",
        "    for i in range(len(diag)):\n",
        "      if diag[i] == 0:\n",
        "        prec = prec + 0\n",
        "      else:\n",
        "        prec = prec + (diag[i])/(np.sum(x[:,i]))\n",
        "    precision = prec / len(diag)\n",
        "    return precision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEV18Ka6ExT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ConfusionMatrix(y_true,y_pred):\n",
        "    \n",
        "    \"\"\"\n",
        "    :type y_true: numpy.ndarray\n",
        "    :type y_pred: numpy.ndarray\n",
        "    :rtype: float\n",
        "    \"\"\"  \n",
        "    #y_true is a list of list after undergoing split function.  \n",
        "    y_true = [i[0] for i in y_true]\n",
        "    number_of_classes = len(np.unique(y_true))\n",
        "    inter_res_1 = np.add(y_pred,(np.multiply(y_true, number_of_classes)))\n",
        "    inter_res_2 = np.histogram(inter_res_1, bins=np.arange(number_of_classes+1, pow(number_of_classes,2)+number_of_classes+2))\n",
        "    Conf_matrix = inter_res_2[0].reshape(number_of_classes,number_of_classes)\n",
        "    return Conf_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbmKI6HPE3Fx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize2(X):\n",
        "  import sklearn\n",
        "  from sklearn.preprocessing import MinMaxScaler\n",
        "  scaler = MinMaxScaler()\n",
        "  scaler.fit(X)\n",
        "  norm_X = scaler.transform(X)\n",
        "  return norm_X\n",
        "\n",
        "\n",
        "def normalize1(X_train):\n",
        "  #MinMaxScaler\n",
        "  print(X_train)\n",
        "  for data in X_train:\n",
        "    min_val = np.min(X_train[data])\n",
        "    #print(min_val)\n",
        "    max_val = np.max(X_train[data])\n",
        "    X[data] = (X_train[data]-min_val)/(max_val-min_val)\n",
        "  X=X.to_numpy()\n",
        "  return X\n",
        "\n",
        "def normalize_preprocess1(X_train, X_test):\n",
        "  #using np.split\n",
        "  Combined = np.append(X_train, X_test, axis = 0)\n",
        "  #print(Combined.type)\n",
        "  Combined = normalize2(Combined)\n",
        "  X_train1 = np.split(Combined,[0,X_train.shape[0]], axis = 0)\n",
        "  X_train = X_train1[1]\n",
        "  X_test = X_train1[2]\n",
        "  return X_train,X_test\n",
        "\n",
        "\n",
        "def Preprocessing(X_train, X_test, Y_train, Y_test):\n",
        "    X_train = X_train.to_numpy()\n",
        "    X_test = X_test.to_numpy()\n",
        "    Y_train = Y_train.to_numpy()\n",
        "    Y_test = Y_test.to_numpy()\n",
        "    return X_train, X_test, Y_train, Y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMXofQIwE6MK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function that calculates euclidean distance using tile\n",
        "def euclidean(X_train,center):\n",
        "    z = pd.np.tile(center,(len(X_train),1))\n",
        "    ans = np.subtract(X_train,z)\n",
        "    ans = pow(ans,2)\n",
        "    ans = np.sum(ans, axis = 1)\n",
        "    ans = np.sqrt(ans)\n",
        "    return ans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKsmTgxLE_bx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#K-NEAREST NEIGHBOR FUNCTION\n",
        "def KNN(X_train,X_test,Y_train,K):\n",
        "    \"\"\"\n",
        "    :type X_train: numpy.ndarray\n",
        "    :type X_test: numpy.ndarray\n",
        "    :type Y_train: numpy.ndarray\n",
        "    \n",
        "    :rtype: numpy.ndarray\n",
        "    \"\"\"\n",
        "    X_train, X_test = normalize_preprocess1(X_train, X_test)\n",
        "    i=0\n",
        "    y_pred = []\n",
        "    correct = 0\n",
        "    m = len(X_train)\n",
        "    for test_sample in X_test:\n",
        "      ans = euclidean(X_train,test_sample)\n",
        "      index_neighbour = np.argpartition(ans, K)[:K]\n",
        "      pred = []\n",
        "      for ind in index_neighbour:\n",
        "        pred.append(Y_train[ind][0])\n",
        "      pred_val =  max(pred, key = pred.count)\n",
        "      y_pred.append(pred_val)\n",
        "    return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2hst33bFExQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#PCA IMPLEMENTATION\n",
        "def PCA(X_train,N):\n",
        "    \"\"\"\n",
        "    :type X_train: numpy.ndarray\n",
        "    :type N: int\n",
        "    :rtype: numpy.ndarray\n",
        "    \"\"\"\n",
        "    X_train = normalize2(X_train)\n",
        "    M = mean(X_train.T, axis = 1)\n",
        "    C = X_train - M\n",
        "    V = cov(C.T)\n",
        "    eig_val_cov, eig_vec_cov = np.linalg.eig(V)\n",
        "    eig_pairs = [(np.abs(eig_val_cov[i]), eig_vec_cov[:,i]) for i in range(len(eig_val_cov))]\n",
        "    eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
        "    W = []\n",
        "    for n in eig_pairs[0][1].tolist():\n",
        "        W.append([n])\n",
        "\n",
        "    for i in range(1, N):\n",
        "        npstack(W, eig_pairs[i][1].tolist())\n",
        "\n",
        "    T = X_train.dot(W)\n",
        "\n",
        "    return T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxvxw76VFIFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def WCSS(clustered_data):\n",
        "    sq_sum = 0\n",
        "    centers = []\n",
        "    for i in range(k):\n",
        "        centers.append(np.mean(clustered_data[i],axis = 0)) \n",
        "    for i in range(k):\n",
        "        sum1 = euclidean(clustered_data[i], centers[i])\n",
        "        sq_sum = sq_sum + np.sum(sum1,axis = 0)\n",
        "    return sq_sum  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5UQ89BTFJ3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#KMEANS IMPLEMENTATION\n",
        "def Kmeans(X_train, X_test, Y_train, k):\n",
        "    \"\"\"\n",
        "    :type X_train: numpy.ndarray\n",
        "    :type N: int\n",
        "    :rtype: List[numpy.ndarray]\n",
        "    \"\"\"\n",
        "    X_train, X_test = normalize_preprocess1(X_train, X_test)\n",
        "    n=X_train.shape[0] #number of training examples\n",
        "    c=X_train.shape[1] #number of features\n",
        "    mean = np.mean(X_train, axis = 0)\n",
        "    std = np.std(X_train, axis = 0)\n",
        "    centers = np.random.randn(k,c)*std + mean #Generate random centers\n",
        "    centers_old = np.ones(centers.shape)\n",
        "    centers_new = deepcopy(centers) # Store new centers\n",
        "    clusters = np.ones(n)\n",
        "    distances = np.ones((n,k))\n",
        "    chistances = np.ones((n,k))\n",
        "    error = np.linalg.norm(centers_new - centers_old)\n",
        "\n",
        "    while error != 0:\n",
        "        for i in range(k):\n",
        "            distances[:,i] = euclidean(X_train,centers_new[i])\n",
        "        clusters = np.argmin(distances, axis = 1)\n",
        "        centers_old = deepcopy(centers_new)\n",
        "        # Calculate mean for every cluster and update the center\n",
        "        for i in range(k):\n",
        "            if (len(X_train[clusters == i]) != 0):\n",
        "                centers_new[i] = np.mean(X_train[clusters == i], axis=0)\n",
        "        error = np.linalg.norm(centers_new - centers_old)\n",
        "    for i in range (k):\n",
        "      clustered_data.append(data[clusters == i])    \n",
        "    return clustered_data   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM89mYezFUMX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SklearnSupervisedLearning(X_train,Y_train,X_test, Y_test):\n",
        "    \"\"\"\n",
        "    :type X_train: numpy.ndarray\n",
        "    :type X_test: numpy.ndarray\n",
        "    :type Y_train: numpy.ndarray\n",
        "    \n",
        "    :rtype: List[numpy.ndarray] \n",
        "    \"\"\"\n",
        "\n",
        "    #Added Y_test to the parameters\n",
        "    X_train, X_test = normalize_preprocess1(X_train, X_test)\n",
        "    clf_knn = KNeighborsClassifier(n_neighbors=7)\n",
        "    clf_knn.fit(X_train, Y_train)\n",
        "    y_pred=clf_knn.predict(X_test)\n",
        "    print(\"Accuracy of KNN: \", Accuracy(Y_test,y_pred))\n",
        "\n",
        "\n",
        "    clf = LogisticRegression(random_state=0).fit(X_train, Y_train)\n",
        "    y_pred_log = clf.predict(X_test)\n",
        "    print(\"Accuracy of Logistic Regression: \", Accuracy(Y_test,y_pred_log))\n",
        "  \n",
        "    clf1 = DecisionTreeClassifier(random_state=0).fit(X_train,Y_train)\n",
        "    y_pred_dec = clf1.predict(X_test)\n",
        "    print(\"Accuracy of Decision Tree Classifier: \", Accuracy(Y_test,y_pred_dec))\n",
        "\n",
        "    clf2 = SVC(gamma='auto').fit(X_train,Y_train)\n",
        "    y_pred_svc = clf2.predict(X_test)\n",
        "    print(\"Accuracy of Support Vector Classifier: \", Accuracy(Y_test,y_pred_dec))\n",
        "\n",
        "    #Plots for confusion matrix\n",
        "    figure, [[ax1, ax2], [ax3, ax4]] = plt.subplots(2,2)\n",
        "    figure.set_figheight(20)\n",
        "    figure.set_figwidth(20)\n",
        "    \n",
        "    \n",
        "    conf_knn = ConfusionMatrix(Y_test,y_pred)\n",
        "    ax1.imshow(conf_knn, cmap = 'Blues')\n",
        "    for (i, j), z in np.ndenumerate(conf_knn):\n",
        "      ax1.annotate(str(z),xy=(j,i), ha='center', va='center')\n",
        "    \n",
        "    ax1.set_xticks(np.arange(1, 11))\n",
        "    ax1.set_yticks(np.arange(1, 11))\n",
        "    ax1.set_xlabel('True Label')\n",
        "    ax1.set_ylabel('Predicted Label')\n",
        "    ax1.set_title('Confusion Matrix of KNN')\n",
        "\n",
        "\n",
        "    conf_log = ConfusionMatrix(Y_test,y_pred_log)\n",
        "    ax2.matshow(conf_log, cmap = 'Blues')\n",
        "    for (i1, j1), z2 in np.ndenumerate(conf_log):\n",
        "      ax2.annotate(str(z2),xy=(j1,i1), ha='center', va='center')\n",
        "    ax2.set_xticks(np.arange(1, 11))\n",
        "    ax2.set_yticks(np.arange(1, 11))\n",
        "    ax2.set_xlabel('True Label')\n",
        "    ax2.set_ylabel('Predicted Label')\n",
        "    ax2.set_title('Confusion Matrix of Logistic Regression')\n",
        "    \n",
        "    \n",
        "    conf_dtc = ConfusionMatrix(Y_test,y_pred_dec)\n",
        "    ax3.matshow(conf_dtc, cmap = 'Blues')\n",
        "    for (i3, j3), z3 in np.ndenumerate(conf_dtc):\n",
        "      ax3.annotate(str(z3),xy=(j3,i3), ha='center', va='center')\n",
        "    ax3.set_xticks(np.arange(1, 11))\n",
        "    ax3.set_yticks(np.arange(1, 11))\n",
        "    ax3.set_xlabel('True Label')\n",
        "    ax3.set_ylabel('Predicted Label')\n",
        "    ax3.set_title('Confusion Matrix of Decision Tree')\n",
        "    \n",
        "    \n",
        "    conf_svm = ConfusionMatrix(Y_test,y_pred_svc)\n",
        "    ax4.matshow(conf_svm, cmap = 'Blues')\n",
        "    for (i4, j4), z4 in np.ndenumerate(conf_svm):\n",
        "      ax4.annotate(str(z4),xy=(j4,i4), ha='center', va='center')\n",
        "    ax4.set_xticks(np.arange(1, 11))\n",
        "    ax4.set_yticks(np.arange(1, 11))\n",
        "    ax4.set_xlabel('True Label')\n",
        "    ax4.set_ylabel('Predicted Label')\n",
        "    ax4.set_title('Confusion Matrix of SVM')\n",
        "    plt.show\n",
        "\n",
        "    lst_predictions = [y_pred, y_pred_log, y_pred_dec, y_pred_svc]\n",
        "    return lst_predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcusC6NsFbWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SklearnVotingClassifier(X_train,Y_train,X_test, Y_test):\n",
        "    \n",
        "    \"\"\"\n",
        "    :type X_train: numpy.ndarray\n",
        "    :type X_test: numpy.ndarray\n",
        "    :type Y_train: numpy.ndarray\n",
        "    \n",
        "    :rtype: List[numpy.ndarray] \n",
        "    \"\"\"\n",
        "    X_train, X_test = normalize_preprocess1(X_train, X_test)\n",
        "    Y_train = Y_train.reshape(Y_train.shape[0],)\n",
        "    Y_test = Y_test.reshape(Y_test.shape[0],)\n",
        "    X_scaled = preprocessing.scale(X_train)\n",
        "    lst_of_classifiers= []\n",
        "    clf_knn = KNeighborsClassifier(n_neighbors=7)\n",
        "    lst_of_classifiers.append(('knn',clf_knn))\n",
        "    clf = LogisticRegression(random_state=0)\n",
        "    lst_of_classifiers.append(('Logistic', clf))\n",
        "    clf1 = DecisionTreeClassifier(random_state=0)\n",
        "    lst_of_classifiers.append(('Decision', clf1))\n",
        "    clf2 = SVC(gamma='auto')\n",
        "    lst_of_classifiers.append(('SVM', clf2)) \n",
        "    ensemble_mod = VotingClassifier(lst_of_classifiers)\n",
        "    classifier3 = ensemble_mod.fit(X_train, Y_train)\n",
        "    y_pred = classifier3.predict(X_test)\n",
        "    print(\"Accuracy of ensemble model: \", Accuracy(Y_test,y_pred))\n",
        "    return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O71hPc2RFjXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Hyperparameters : Number of neighbours, metric used for calculation of distance\n",
        "def GridSearchKnn(X_train,Y_train):\n",
        "  X_train = normalize2(X_train)\n",
        "  neighbours = [3, 5, 7]\n",
        "  grid_params = {'n_neighbors' : neighbours, 'metric' : ['euclidean','manhattan']}\n",
        "  Y_train = Y_train.reshape(Y_train.shape[0],)\n",
        "  clf = GridSearchCV(KNeighborsClassifier(), grid_params, scoring='accuracy')\n",
        "  model = clf.fit(X_train, Y_train)\n",
        "  print('Best number of neighbours for KNN: ',model.best_estimator_.get_params()['n_neighbors'])\n",
        "  print('Best distance for KNN: ',model.best_estimator_.get_params()['metric'])\n",
        "  pvt = pd.pivot_table(pd.DataFrame(model.cv_results_),values='mean_test_score', index = 'param_n_neighbors', columns = 'param_metric')\n",
        "  #pvt = pd.pivot_table(pd.DataFrame(model.cv_results_),columns = 'param_metric')  \n",
        "  ax = sns.heatmap(pvt,annot=True)\n",
        "  ax1 = pvt.plot()\n",
        "  ax1.set_xlabel(\"Number of Neighbors\")\n",
        "  ax1.set_ylabel(\"Accuracy\")\n",
        "  return pvt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPNFJn6WFs71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Hypeparameters : Depth, impurity calculation method\n",
        "def GridSearchDTC(X_train, Y_train):\n",
        "  X_train = normalize2(X_train)\n",
        "  depth = np.arange(8,12)\n",
        "  #params_dtc = {'max_depth': depth}\n",
        "  params_dtc = {'max_depth': depth,'criterion':['gini','entropy']}\n",
        "  clf_dtc = GridSearchCV(DecisionTreeClassifier(), params_dtc, scoring='accuracy')\n",
        "  model_dtc = clf_dtc.fit(X_train, Y_train)\n",
        "  print('Maximum Depth: ',model_dtc.best_estimator_.get_params()['max_depth'])\n",
        "  print('Criterion to calculate impurity:',model_dtc.best_estimator_.get_params()['criterion'])\n",
        "  pvt = pd.pivot_table(pd.DataFrame(model_dtc.cv_results_),values='mean_test_score', index = 'param_max_depth', columns = 'param_criterion')\n",
        "  ax = sns.heatmap(pvt,annot=True)\n",
        "  ax1 = pvt.plot()\n",
        "  ax1.set_xlabel(\"Depth\")\n",
        "  ax1.set_ylabel(\"Accuracy\")\n",
        "  return pvt "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ2QIwcHFuiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Hyperparameters : Regularization parameter \n",
        "def GridSearchSvm(X_train,Y_train):\n",
        "  X_train = normalize2(X_train)\n",
        "  regularization = [0.1,0.5,1,10]\n",
        "  Y_train = Y_train.reshape(Y_train.shape[0],)\n",
        "  grid_params_svm = {'C' : regularization}\n",
        "  clf_svm = GridSearchCV(svm.SVC(kernel='rbf'), grid_params_svm, scoring='accuracy')\n",
        "  model_svm = clf_svm.fit(X_train, Y_train)\n",
        "  print('Best regularization for SVM',model_svm.best_estimator_.get_params()['C'])\n",
        "  lst_mean_scores_svm = model_svm.cv_results_['mean_test_score']\n",
        "  plt.plot(regularization, lst_mean_scores_svm, linestyle='solid')\n",
        "  plt.xlabel('Regularization')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w22cweCXEGdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We have used MinMax scaling as the models perform better under this scaling. \n",
        "#Therefore, the normalize function is called within the functions and the data needn't be normalized before being fed into the functions. \n",
        "dfs = pd.read_csv('data.csv',encoding='unicode_escape')\n",
        "X = dfs.iloc[:,:48]\n",
        "y = dfs.iloc[:,48:]\n",
        "\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=0.2)\n",
        "X_train, X_test, Y_train, Y_test = Preprocessing(X_train,X_test,Y_train,Y_test) \n",
        "X_train\n",
        "\n",
        "PCA(X_train,3)\n",
        "\n",
        "y_pred = KNN(X_train,X_test,Y_train, 5)\n",
        "print(ConfusionMatrix(Y_test, y_pred))\n",
        "print(Accuracy(Y_test,y_pred))\n",
        "print(Recall(Y_test,y_pred))\n",
        "print(Precision(Y_test,y_pred))\n",
        "\n",
        "Kmeans(X_train, X_test, Y_train, 11)\n",
        "\n",
        "\n",
        "SklearnSupervisedLearning(X_train,Y_train,X_test, Y_test)\n",
        "\n",
        "SklearnVotingClassifier(X_train,Y_train,X_test, Y_test)\n",
        "\n",
        "GridSearchKnn(X_train,Y_train)\n",
        "\n",
        "GridSearchDTC(X_train,Y_train)\n",
        "\n",
        "GridSearchSvm(X_train,Y_train)\n",
        "\n",
        "#K-Nearest Neighbour implementation\n",
        "y_pred = KNN(X_train,X_test,Y_train, 5)\n",
        "print(ConfusionMatrix(Y_test, y_pred))\n",
        "print(Accuracy(Y_test,y_pred))\n",
        "print(Recall(Y_test,y_pred))\n",
        "print(Precision(Y_test,y_pred))\n",
        "\n",
        "#X_train1 = normalize1(X_train)\n",
        "#shape of training data\n",
        "m = X_train.shape\n",
        "print(X_train)\n",
        "print(m[0])\n",
        "n = X_test.shape\n",
        "print(n[0])\n",
        "\n",
        "\n",
        "Combined = np.append(X_train, X_test, axis = 0)\n",
        "Combined = normalize1(Combined)\n",
        "X_train1 = np.split(Combined,[0,X_train.shape[0]], axis = 0)\n",
        "X_train = X_train1[1]\n",
        "X_test = X_train1[2]\n",
        "\n",
        "print(\"New shape: \", X_train.shape)\n",
        "print(\"New shape: \", X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}